---
title: "Orchestration File"
format:
  html:
    code-fold: true
    number-sections: true
---

The orchestration file `_main.qmd` can be used to sequentally compile individual subprojects from a single space.

The project is composed of multiple subprojects and is divided into two main sections.
The first section entails the execution of code to obtain raw data from publicly available sources and conduct initial preprocessing.
Conversely, the second section builds upon the preprocessed data, centering on analysis and reporting.
The connection between the two sections is facilitated by a _pinboard_ of data, which is exported in @sec-data and reimported in @sec-analysis.
Consequently, solely the project administrator is responsible for running the first section, while collaborators can concentrate on the subprojects within section two.


Several environmental variables need to be defined in `.Revinron` file.
For working exclusively in @sec-analysis, only the _url_ of the _pinboard_ needs to be defined.


## Data {#sec-data}

This section is intended for obtaining data, processing data, and producing topic models.
All relevant outputs get exported into their respective pinboards.

[Code in this section needs to be run only by the project administrators.]{style="color: red"}

### Get raw data {#sec-data_raw}

Here we obtain data from various sources. Because the sources - APIs, websites, url addresses of files - are external to the project and not persistent, it is unlikely that the code will be reproducible in the longterm. Hence, the main purpose of this section is to document the process of how certain datasets were originally obtained, but it is not expected that researchers will run this code repeatedly.

#### Obtain raw data programatically

This sub-section contains functions tailored to specific APIs and websites to be scraped. These might be challenging to run and the execution can take hours or days. As a safeguard against accidental trigger, an evironmental variable has to be explicitly set to execute the code.

- __Subproject name__:
  - `data_api` 
- __Requirements__: 
  - Set `API_RUN=TRUE` in `.Renviron` to call APIs.

```{r}
#| label: data_get_api
#| eval: false
quarto::quarto_render("_data_api.qmd", quiet = TRUE)
targets::tar_make(script = "_data_api.R", store = "_data_api")
```

#### Obtain raw data by download {#sec-data_get}

The code in this subsection obtains those datasets that need not be collected via APIs or scraping, because they are distributed as files that can be directly downloaded. Setting url addresses for downloads is therefore necessary. Data are simply downloaded and kept as raw data. Processing is documented separarely in @sec-data_process.

- __Subproject name__:
  - `data_get` 
- __Requirements__: 
  - Set `URL_CSF=URL` in `.Renviron`.
  - Set `URL_ANR_2010=URL` in `.Renviron`.
  - Set `URL_ANR_2009=URL` in `.Renviron`.
  - Set `URL_CSF=URL` in `.Renviron`.
  - Set `URL_ANR_2010=URL` in `.Renviron`.
  - Set `URL_ANR_2010_P=URL` in `.Renviron`.
  - Set `URL_ANR_2009=URL` in `.Renviron`.
  - Set `URL_ANR_2009_P=URL` in `.Renviron`.
  - Set `URL_EU_EUROPE=URL` in `.Renviron`.
  - Set `URL_EU_2020=URL` in `.Renviron`.
  - Set `URL_EU_FP7=URL` in `.Renviron`.
  - Set `URL_SNSF=URL` in `.Renviron`.


```{r}
#| label: data_get_downloads 
#| eval: false
quarto::quarto_render("_data_get.qmd", quiet = TRUE)
targets::tar_make(script = "_data_get.R", store = "_data_get")
```

### Process data {#sec-data_process}

- __Subproject name__:
  - `data_process` 
- __Requirements__: 
  - @sec-data_get must be completed.
  
This code needs to be run only if data sources change and need to be updated. You should only compile this subproject if you know what you are doing and why you are doing it.

```{r}
#| label: data_export
#| eval: false
quarto::quarto_render("_data_process.qmd", quiet = TRUE)
targets::tar_make(script = "_data_process.R", store = "_data_process")
```

### Build topic model {#sec-data_topmodel}

- __Subproject name__:
  - `data_topmodel` 
- __Requirements__: 
  - @sec-data_process must be completed.


### Export pins

- __Subproject name__:
  - `data_export` 
- __Requirements__: 
  -  @sec-data_get must be completed.
  -  @sec-data_process must be completed.
  -  @sec-data_topmodel must be completed.
  - Set `PINS_BOARD=PATH_TO_PINBOARD` in `.Renviron`.

This code needs to be run only if data sources change and need to be updated. In principle, this is not required except when initializing the project. 


## Analysis (default) {#sec-analysis}

- __Subproject name__:
  - `main` 
- __Requirements__:
  - See requirements for @sec-data_import and @sec-data_analyze.

This is the default pipeline. It serves for analysis of the raw data previously exported onto a pinboard. 

Code in this section should be run by all collaborators, on all machines. It is connected to the previous section only by dependency on the generated pins.

### (Re)Import pins {#sec-data_import}

Here we reimport data processed and produced in @sec-data

- __Subproject name__:
  - `data_import` 
- __Requirements__:
  -  Set `MANIFEST_URL=URL/_pins.yaml` in `.Renviron`. 
  - internet connection.

```{r}
#| label: data_import
#| eval: false
quarto::quarto_render("_data_import.qmd", quiet = TRUE)
targets::tar_make(script = "_data_import.R", store = "_data_import")
```

### Analyze data {#sec-data_analyze}

- __Subproject name__:
  - `data_analyze` 
- __Requirements__: 
  -  @sec-data_import must be completed

```{r}
#| label: quarto-projects
#| eval: false
quarto::quarto_render("_data_analyze.qmd", quiet = TRUE)
targets::tar_make(script = "_data_analyze.R", store = "_data_analyze")
```


### Report results

### Publish docs


```{r}
#| label: publish-docs
#| eval: false
#From: https://gist.github.com/cobyism/4730490
system("quarto render")
system("touch _book/.nojekyll")
system("git add . && git commit -m 'Update gh-pages'")
system("git subtree push --prefix _book origin gh-pages")
```

