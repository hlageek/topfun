[
  {
    "objectID": "index.html#acknowlegements",
    "href": "index.html#acknowlegements",
    "title": "TOPFUN Project Documentation",
    "section": "0.1 Acknowlegements",
    "text": "0.1 Acknowlegements\nThe collaboration was initiated thanks to a joint Barrande mobility for ELICO project and CSTSS project."
  },
  {
    "objectID": "README.html#instructions",
    "href": "README.html#instructions",
    "title": "2  README.md",
    "section": "2.1 Instructions",
    "text": "2.1 Instructions\nTo reproduce the project, run the following:\n\nin Terminal:\n\ngit clone https://github.com/hlageek/funding-topics.git to clone the repository\n\nin R console:\n\ninstall.packages(\"renv\") to enable renv dependencies management for the project\nrenv::restore()` to restore the package environment of the project\nquarto::quarto_render(as_job = FALSE) to render all quarto-defined pipelines for targets\ntargets::tar_make() to compile analytical pipelines and generate contents of the Outputs folder\n\n\nThe scripts/main.qmd quarto file offers a more finegrained control over the exectution of individual project pipelines.\nEach pipeline has its own quarto file located in the subfolders of scripts folder.\nAutomatically generated files and folders as well as essential helper and configuration files and folders have names beginning with underscore: _. The necessary configuration for the repository is defined in _quarto.yml for quarto and _targets.yaml for targets."
  },
  {
    "objectID": "README.html#structure-of-the-repository",
    "href": "README.html#structure-of-the-repository",
    "title": "2  README.md",
    "section": "2.2 Structure of the repository",
    "text": "2.2 Structure of the repository\nThe repository is structured according to TIER Protocol 4.0 specification for conducting and documenting an empirical research project with additional requirements for quarto and targets projects to construct a reproducible project environment."
  },
  {
    "objectID": "scripts/main.html#sec-data",
    "href": "scripts/main.html#sec-data",
    "title": "3  Orchestration File",
    "section": "3.1 Data",
    "text": "3.1 Data\nThis section is intended for obtaining data, processing data, and producing topic models. All relevant outputs get exported into their respective pinboards.\nCode in this section needs to be run only by the project administrators.\n\n3.1.1 Obtain raw data\nHere we obtain data from various sources. Because the sources - APIs, websites, url addresses of files - are external to the project and not persistent, it is unlikely that the code will be reproducible in the longterm. Hence, the main purpose of this section is to document the process of how certain datasets were originally obtained, but it is not expected that researchers will run this code repeatedly.\n\n3.1.1.1 Obtain raw data programatically\n\n\nCode\n# Define the paths to the pipeline generator and the generated pipeline\nscripts_data_get_api &lt;- here::here(\n  \"scripts\",\n  \"data_collection_scripts\",\n  c(\n    \"data_api.qmd\",\n    \"_data_api.R\"\n  )\n)\n# Generate the pipeline using the Quarto file\nquarto::quarto_render(scripts_data_get_api[1], quiet = TRUE)\n# Run the generated pipeline\ntargets::tar_make(script = scripts_data_get_api[2], store = \"_store/_data_api\")\n\n\nThis sub-section contains functions tailored to specific APIs and websites to be scraped. These might be challenging to run and the execution can take hours or days. As a safeguard against accidental trigger, an evironmental variable has to be explicitly set to execute the code.\n\nSubproject name:\n\ndata_api\n\nRequirements:\n\nSet API_RUN=TRUE in .Renviron to call APIs.\n\n\n\n\n3.1.1.2 Obtain raw data by download\n\n\nCode\n# Define the paths to the pipeline generator and the generated pipeline\nscripts_data_get_downloads &lt;- here::here(\n  \"scripts\",\n  \"data_collection_scripts\",\n  c(\n    \"data_get.qmd\",\n    \"_data_get.R\"\n  )\n)\n# Generate the pipeline using the Quarto file\nquarto::quarto_render(scripts_data_get_downloads[1], quiet = TRUE)\n# Run the generated pipeline\ntargets::tar_make(script = scripts_data_get_downloads[2], store = \"_store/_data_get\")\n\n\nThe code in this subsection obtains those datasets that need not be collected via APIs or scraping, because they are distributed as files that can be directly downloaded. Setting url addresses for downloads is therefore necessary. Data are simply downloaded and kept as raw data. Processing is documented separarely in Section 3.1.2.\n\nSubproject name:\n\ndata_get\n\nRequirements:\n\nSet URL_CSF=URL in .Renviron.\nSet URL_ANR_2010=URL in .Renviron.\nSet URL_ANR_2009=URL in .Renviron.\nSet URL_CSF=URL in .Renviron.\nSet URL_ANR_2010=URL in .Renviron.\nSet URL_ANR_2010_P=URL in .Renviron.\nSet URL_ANR_2009=URL in .Renviron.\nSet URL_ANR_2009_P=URL in .Renviron.\nSet URL_EU_EUROPE=URL in .Renviron.\nSet URL_EU_2020=URL in .Renviron.\nSet URL_EU_FP7=URL in .Renviron.\nSet URL_SNSF=URL in .Renviron.\n\n\n\n\n\n3.1.2 Process data\n\n\nCode\n# Define the paths to the pipeline generator and the generated pipeline\nscripts_data_process &lt;- here::here(\n  \"scripts\",\n  \"processing_scripts\",\n  c(\"data_process.qmd\", \"_data_process.R\"))\n# Generate the pipeline using the Quarto file\nquarto::quarto_render(scripts_data_process[1], quiet = TRUE)\n# Run the generated pipeline\ntargets::tar_make(script = scripts_data_process[2], store = \"_store/_data_process\")\n\n\n\nSubproject name:\n\ndata_process\n\nRequirements:\n\nSection 3.1.1.2 must be completed.\nSection 3.1.1.1 must be completed.\n\n\nThis code needs to be run only if data sources change and need to be updated. You should only compile this subproject if you know what you are doing and why you are doing it.\n\n\n3.1.3 Build topic model\n\nSubproject name:\n\ndata_topmodel\n\nRequirements:\n\nSection 3.1.2 must be completed.\n\n\n\n\n3.1.4 Export pins\n\nSubproject name:\n\ndata_export\n\nRequirements:\n\nSection 3.1.1.2 must be completed.\nSection 3.1.2 must be completed.\nSection 3.1.3 must be completed.\nSet PINS_BOARD=PATH_TO_PINBOARD in .Renviron.\n\n\nThis code needs to be run only if data sources change and need to be updated. In principle, this is not required except when initializing the project."
  },
  {
    "objectID": "scripts/main.html#sec-analysis",
    "href": "scripts/main.html#sec-analysis",
    "title": "3  Orchestration File",
    "section": "3.2 Analysis (default)",
    "text": "3.2 Analysis (default)\nCode in this section should be run by all collaborators, on all machines. It is connected to the previous section only by dependency on the generated pins.\nThis is the default pipeline. It serves for analysis of the raw data previously exported onto a pinboard.\n\n\nCode\n# Generate all pipelines using the Quarto project\n# defined in _quarto.yaml\nquarto::quarto_render(as_job = FALSE)\n# Run the generated analytical pipelines\ntargets::tar_make()\n\n\n\nSubproject name:\n\nmain\n\nRequirements:\n\nSee requirements for Section 3.2.1 and Section 3.2.2.\n\n\n\n3.2.1 (Re)Import pins\n\n\nCode\n# Define the paths to the pipeline generator and the generated pipeline\nscripts_data_import &lt;- here::here(\n  \"scripts\",\n  \"analysis_scripts\",\n  c(\"data_import.qmd\", \"_data_import_.R\"))\n# Generate the pipeline using the Quarto file\nquarto::quarto_render(\"data_import.qmd\", quiet = TRUE)\n# Run the generated pipeline\ntargets::tar_make(script = \"_data_import.R\", store = \"_store/_data_import\")\n\n\nHere we reimport data processed and produced in Section 3.1\n\nSubproject name:\n\ndata_import\n\nRequirements:\n\nSet MANIFEST_URL=URL/_pins.yaml in .Renviron.\ninternet connection.\n\n\n\n\n3.2.2 Analyze data\n\nSubproject name:\n\ndata_analyze\n\nRequirements:\n\nSection 3.2.1 must be completed\n\n\n\n\nCode\n# Define the paths to the pipeline generator and the generated pipeline\nscripts_data_import &lt;- here::here(\n  \"scripts\",\n  \"analysis_scripts\",\n  c(\"data_analyze.qmd\", \"_data_analyze.R\"))\n# Generate the pipeline using the Quarto file\nquarto::quarto_render(\"data_analyze.qmd\", quiet = TRUE)\n# Run the generated pipeline\ntargets::tar_make(script = \"_data_analyze.R\", store = \"_store/_data_analyze\")\n\n\n\n\n3.2.3 Report results\n\n\n3.2.4 Publish docs\n\n\nCode\n#From: https://gist.github.com/cobyism/4730490\nquarto::quarto_render(as_job = FALSE)\nsystem(\"touch _book/.nojekyll\")\nsystem(\"git add . && git commit -m 'Update gh-pages'\")\nsystem(\"git subtree push --prefix _book origin gh-pages\")"
  },
  {
    "objectID": "scripts/data_collection_scripts/data_api.html#setup",
    "href": "scripts/data_collection_scripts/data_api.html#setup",
    "title": "4  Obtain raw data programatically",
    "section": "4.1 Setup",
    "text": "4.1 Setup\n\n4.1.1 Set environment for subproject\n\nSys.setenv(TAR_PROJECT = \"data_api\")\n\n\n\n4.1.2 Initialize targets script\n\nlibrary(targets)\ntar_unscript()\nknitr::opts_chunk$set(collapse = TRUE, comment = \"#&gt;\", tar_script = \"_data_api.R\")\n\n\n\n4.1.3 Define targets globals\nWe first define some global options/functions common to all targets.\n\n# read custom functions\nlapply(list.files(here::here(\"R\"), full.names = TRUE), source)\ntar_option_set(packages = packages())\n#&gt; Establish _data_api.R and _data_api_r/globals/globals.R."
  },
  {
    "objectID": "scripts/data_collection_scripts/data_api.html#targets",
    "href": "scripts/data_collection_scripts/data_api.html#targets",
    "title": "4  Obtain raw data programatically",
    "section": "4.2 Targets",
    "text": "4.2 Targets\n\n4.2.1 DFG - Gepris\nGepris information page (https://gepris.dfg.de/gepris/OCTOPUS?task=showMonitor) at scrape time:\nData Monitor Last update: 26.01.2024\nMost recent date of approval: 26.11.2023 All projects decided upon and approved between 1.1.1999 and the specified award letter date can be found in GEPRIS.\nEntries by Type Count Projects with final reports 40901 Projects 142517 Persons 93668 Institutions 43769 Entries by Research Area Count Humanities and Social Sciences Colour index for Humanities and Social Sciences 26304 Life Sciences Colour index for Life Sciences 49480 Natural Sciences Colour index for Natural Sciences 38446 Engineering Sciences Colour index for Engineering Sciences 26304 Infrastructure 317\n\nlist(\n  tar_target(\n    name = dfg_catalogue_tsv,\n    command = scrape_dfg_catalogue(\n      gepris_url = \"https://gepris.dfg.de/gepris/OCTOPUS?beginOfFunding=&bewilligungsStatus=&bundesland=DEU%23&context=projekt&einrichtungsart=-1&fachgebiet=%23&findButton=historyCall&gefoerdertIn=&ggsHunderter=0&hitsPerPage=50&index=1&nurProjekteMitAB=false&oldGgsHunderter=0&oldfachgebiet=%23&pemu=24&peu=%23&task=doKatalog&teilprojekte=true&zk_transferprojekt=false\",\n      outfile = here::here(\"data\", \"input_data\", \"dfg_catalogue.tsv\"), \n      sleep_time = 5,\n      run = as.logical(Sys.getenv(\"API_RUN\")) #as.logical(Sys.getenv(\"API_RUN\"))\n      ),\n    format = \"file\"\n  ),\ntar_target(\n    name = dfg_projects_tsv,\n    command = scrape_dfg_projects(\n      gepris_catalogue = dfg_catalogue_tsv,\n      outfile = here::here(\"data\", \"input_data\", \"dfg_projects.tsv\"), \n      sleep_time = 5,\n      run = as.logical(Sys.getenv(\"API_RUN\")) #\n      ),\n    format = \"file\"\n  )\n)\n#&gt; Establish _data_api.R and _data_api_r/targets/dfg.R.\n\n\n\n4.2.2 HAL\n\nlist(\n  tar_target(\n    name = hal_types,\n    command = c(\"ART\", \"COUV\", \"COMM\", \"OUV\")\n  ),\n  tar_target(\n    name = hal_years,\n    command = seq(2008, 2012, 1)\n  ),\n  tar_target(\n    name = hal,\n    command = call_hal(hal_types, hal_years, run = as.logical(Sys.getenv(\"API_RUN\"))),\n    pattern = cross(hal_types, hal_years)\n\n  )\n)\n#&gt; Establish _data_api.R and _data_api_r/targets/hal.R."
  },
  {
    "objectID": "scripts/data_collection_scripts/data_api.html#reset-subproject",
    "href": "scripts/data_collection_scripts/data_api.html#reset-subproject",
    "title": "4  Obtain raw data programatically",
    "section": "4.3 Reset subproject",
    "text": "4.3 Reset subproject\n\nSys.setenv(TAR_PROJECT = \"main\")"
  },
  {
    "objectID": "scripts/data_collection_scripts/data_get.html#setup",
    "href": "scripts/data_collection_scripts/data_get.html#setup",
    "title": "Obtain raw data by downloads",
    "section": "Setup",
    "text": "Setup\n\nTargets environment for subproject\n\nSys.setenv(TAR_PROJECT = \"data_get\")\n\n\n\nTargets script setup\n\nlibrary(targets)\ntar_unscript()\nknitr::opts_chunk$set(tar_script = \"_data_get.R\", collapse = TRUE, comment = \"#&gt;\")\n\n\n\nTargets globals\nWe first define some global options/functions common to all targets.\n\n# read custom functions\nlapply(list.files(here::here(\"R\"), full.names = TRUE), source)\ntar_option_set(packages = packages())\n# obtain configuration from .Renviron\nurl_csf &lt;- Sys.getenv(\"URL_CSF\")\nurl_anr_2010 &lt;- Sys.getenv(\"URL_ANR_2010\")\nurl_anr_2010_p &lt;- Sys.getenv(\"URL_ANR_2010_P\")\nurl_anr_2009 &lt;- Sys.getenv(\"URL_ANR_2009\")\nurl_anr_2009_p &lt;- Sys.getenv(\"URL_ANR_2009_P\")\nurl_eu_europe  &lt;- Sys.getenv(\"URL_EU_EUROPE\")\nurl_eu_2020 &lt;- Sys.getenv(\"URL_EU_2020\")\nurl_eu_fp7 &lt;- Sys.getenv(\"URL_EU_FP7\")\nurl_snsf &lt;- Sys.getenv(\"URL_SNSF\")\n#&gt; Establish _data_get.R and _data_get_r/globals/globals.R."
  },
  {
    "objectID": "scripts/data_collection_scripts/data_get.html#targets",
    "href": "scripts/data_collection_scripts/data_get.html#targets",
    "title": "Obtain raw data by downloads",
    "section": "Targets",
    "text": "Targets\n\nCzech Science Foundation\n\nlist(\n   tarchetypes::tar_download(\n    name = csf_download_files,\n    urls = url_csf,\n    paths = here::here(\"data\", \"input_data\", \"csf.tsv\")\n  )\n)\n#&gt; Establish _data_get.R and _data_get_r/targets/csf.R.\n\n\n\nAgence nationale de la recherche\n\nlist(\n  tarchetypes::tar_download(\n    name = anr_download_files,\n    urls = c(url_anr_2010, url_anr_2009, url_anr_2010_p, url_anr_2009_p),\n    paths = paste0(\n        here::here(\"data\", \"input_data\"), \n        .Platform$file.sep,  \n        c(\"anr_2010\", \"anr_2009\", \"anr_2010_p\", \"anr_2009_p\"), \n        \".csv\")\n  ),\n  tar_target(\n    name = anr_data_raw_2010,\n    command = readr::read_delim(anr_download_files[1],\n                                 delim = \";\",\n                                 locale = locale(\"fr\",\n                                                 decimal_mark = \".\"),\n                                                 show_col_types = FALSE) |&gt; \n                                                 janitor::clean_names()|&gt; \n                                                 dplyr::mutate(source = \"since2010\"),\n    format = \"qs\"\n  ),\n  tar_target(\n    name = anr_data_raw_2009,\n    command = readr::read_delim(anr_download_files[2],\n                                 delim = \";\",\n                                 locale = locale(\"fr\",\n                                                 decimal_mark = \".\"),\n                                show_col_types = FALSE) |&gt; \n                                                 janitor::clean_names() |&gt; \n                                                 dplyr::mutate(source = \"before2009\"),\n    format = \"qs\"\n  ),\n  tar_target(\n    name = anr_data_raw,\n    command = dplyr::bind_rows(\n        anr_data_raw_2010,\n        anr_data_raw_2009\n    ),\n    format = \"qs\"\n  )\n)\n#&gt; Establish _data_get.R and _data_get_r/targets/anr.R.\n\n\n\nCORDIS\n\nlist(\n  tarchetypes::tar_download(\n    name = eu_download_files_zip,\n    urls = c(url_eu_europe, url_eu_2020, url_eu_fp7),\n    paths = paste0(\n        here::here(\"data\", \"input_data\"), \n        .Platform$file.sep,  \n        c(\"eu_europe\", \"eu_horizon\", \"eu_fp7\"), \n        \".zip\"),\n    method = \"curl\",\n    mode = \"wb\"\n  ),\n  tar_target(\n    name = eu_download_files,\n    command = unzip_eu_files(eu_download_files_zip, data_path = here::here(\"data\", \"input_data\")),\n    format = \"file\" \n  ),\ntar_target(\n    name = eu_data_raw,\n    command = purrr::map_df(eu_download_files, read.csv2) |&gt; \n              dplyr::as_tibble(),\n    format = \"qs\" \n  )\n)\n#&gt; Establish _data_get.R and _data_get_r/targets/cordis.R.\n\n\n\nSwiss National Science Foundation\n\nlist(\n  tarchetypes::tar_download(\n    name = snsf_download_files,\n    urls = c(url_snsf),\n    paths = paste0(\n        here::here(\"data\", \"input_data\"), \n        .Platform$file.sep,  \n        c(\"snsf\"), \n        \".csv\"),\n    method = \"curl\",\n    mode = \"w\"\n  ),\ntar_target(\n    name = snsf_data_raw,\n    command = read.csv2(snsf_download_files)|&gt; \n              dplyr::as_tibble(),\n    format = \"qs\" \n  )\n)\n#&gt; Establish _data_get.R and _data_get_r/targets/snsf.R."
  },
  {
    "objectID": "scripts/data_collection_scripts/data_get.html#reset-project",
    "href": "scripts/data_collection_scripts/data_get.html#reset-project",
    "title": "Obtain raw data by downloads",
    "section": "Reset project",
    "text": "Reset project\n\nSys.setenv(TAR_PROJECT = \"main\")"
  },
  {
    "objectID": "scripts/processing_scripts/data_process.html#setup",
    "href": "scripts/processing_scripts/data_process.html#setup",
    "title": "6  Process raw data",
    "section": "6.1 Setup",
    "text": "6.1 Setup\n\n6.1.1 Set environment for subproject\n\nSys.setenv(TAR_PROJECT = \"data_process\")\n\n\n\n6.1.2 Initialize targets script\n\nlibrary(targets)\ntar_unscript()\nknitr::opts_chunk$set(tar_script = \"_data_process.R\", collapse = TRUE, comment = \"#&gt;\")\n\n\n\n6.1.3 Define targets globals\nWe first define some global options/functions common to all targets.\n\n# read custom functions\nlapply(list.files(here::here(\"R\"), full.names = TRUE), source)\ntar_option_set(packages = packages())\n#&gt; Establish _data_process.R and _data_process_r/globals/globals.R."
  },
  {
    "objectID": "scripts/processing_scripts/data_process.html#targets",
    "href": "scripts/processing_scripts/data_process.html#targets",
    "title": "6  Process raw data",
    "section": "6.2 Targets",
    "text": "6.2 Targets\nFirst we register raw data files.\n\nlist(\n  tar_target(\n    name = csf_data_raw,\n    command = targets::tar_read(csf_download_files, store = \"_store/_data_get\"),\n    format = \"file\"\n  ),\n  tar_target(\n    name = anr_data_raw,\n    command = targets::tar_read(anr_download_files, store = \"_store/_data_get\"),\n    format = \"file\"\n  ),\n  tar_target(\n    name = snsf_data_raw,\n    command = targets::tar_read(snsf_download_files, store = \"_store/_data_get\"),\n    format = \"file\"\n  ),\n  tar_target(\n    name = eu_data_raw,\n    command = targets::tar_read(eu_download_files, store = \"_store/_data_get\"),\n    format = \"file\"\n  ),\n  tar_target(\n    name = dfg_data_raw,\n    command = targets::tar_read(dfg_projects_tsv, store = \"_store/_data_api\"),\n    format = \"file\"\n  ),\n  tar_target(\n    name = topfun_data_archive,\n    command = archive_topfun_data(\n      data_files = list(\n        csf_data_raw,\n        anr_data_raw,\n        eu_data_raw,\n        snsf_data_raw,\n        dfg_data_raw\n      ),\n      archive_file = here::here(\"data\", \"intermediate_data\", paste0(\"topfun_data_archive-\", format(Sys.time(), \"%Y%m%d%H%M%S\"), \".zip\"))\n    ),\n    format = \"file\"\n  )\n)\n#&gt; Establish _data_process.R and _data_process_r/targets/register_data.R."
  },
  {
    "objectID": "scripts/processing_scripts/data_process.html#reset-subproject",
    "href": "scripts/processing_scripts/data_process.html#reset-subproject",
    "title": "6  Process raw data",
    "section": "6.3 Reset subproject",
    "text": "6.3 Reset subproject\n\nSys.setenv(TAR_PROJECT = \"main\")"
  },
  {
    "objectID": "scripts/targets.html#setup",
    "href": "scripts/targets.html#setup",
    "title": "7  Targets",
    "section": "7.1 Setup",
    "text": "7.1 Setup\n\n7.1.1 Targets script setup\n\n\n7.1.2 Targets environment for subproject\n\n\n7.1.3 Targets globals\nWe first define some global options/functions common to all targets.\n\n# read custom functions\nlapply(list.files(here::here(\"R\"), full.names = TRUE), source)\ntar_option_set(packages = packages())\n# define function for running sub-pipelines and track their dependencies\nrun_subpipeline &lt;- function(script, store, depend_on){\n        targets::tar_make(\n        script = script,\n        store = store\n        )\n        c(\"Last compiled\" = Sys.time())\n}\n#&gt; Establish _targets.R and _targets_r/globals/globals.R."
  },
  {
    "objectID": "scripts/targets.html#code",
    "href": "scripts/targets.html#code",
    "title": "7  Targets",
    "section": "7.2 Code",
    "text": "7.2 Code\n\nlist(\n tarchetypes::tar_render(\n    name = script_data_import,\n    path = here::here(\"scripts\", \"analysis_scripts\", \"data_import.qmd\"),\n ),\n tar_target(\n    name = compile_data_import,\n    command = run_subpipeline(\n         script = here::here(\"scripts\", \"analysis_scripts\", \"_data_import.R\"), \n         store = \"_store/_data_import\", \n         depend_on = script_data_import\n         )\n ),\n tarchetypes::tar_render(\n    name = script_data_analyze,\n    path = here::here(\"scripts\", \"analysis_scripts\", \"data_analyze.qmd\"),\n ),\n tar_target(\n    name = compile_data_analyze,\n    command = run_subpipeline(\n        script = here::here(\"scripts\", \"analysis_scripts\", \"_data_analyze.R\"), \n         store = \"_store/_data_analyze\", \n         depend_on = script_data_analyze\n         )\n )\n)\n#&gt; Establish _targets.R and _targets_r/targets/code.R."
  },
  {
    "objectID": "scripts/analysis_scripts/data_import.html#setup",
    "href": "scripts/analysis_scripts/data_import.html#setup",
    "title": "8  Import data from pinboard",
    "section": "8.1 Setup",
    "text": "8.1 Setup\n\n8.1.1 Targets environment for subproject\n\n\n8.1.2 Targets script setup\n\n\n8.1.3 Targets globals\nWe first define some global options/functions common to all targets.\n\n# read custom functions\nlapply(list.files(here::here(\"R\"), full.names = TRUE), source)\ntar_option_set(packages = packages())\n#&gt; Establish _data_import.R and _data_import_r/globals/globals.R."
  },
  {
    "objectID": "scripts/analysis_scripts/data_import.html#code",
    "href": "scripts/analysis_scripts/data_import.html#code",
    "title": "8  Import data from pinboard",
    "section": "8.2 Code",
    "text": "8.2 Code\n\nlist(\n    tar_target(\n        name = test3,\n        command = sum(3+9)\n    ),\n    tar_target(\n        name = import_test4,\n        command = sum(3+6)\n    )\n)\n#&gt; Establish _data_import.R and _data_import_r/targets/code.R."
  },
  {
    "objectID": "scripts/analysis_scripts/data_analyze.html#setup",
    "href": "scripts/analysis_scripts/data_analyze.html#setup",
    "title": "9  Analyze data",
    "section": "9.1 Setup",
    "text": "9.1 Setup\n\n9.1.1 Targets environment for subproject\n\n\n9.1.2 Targets script setup\n\n\n9.1.3 Targets globals\nWe first define some global options/functions common to all targets.\n\n# read custom functions\nlapply(list.files(here::here(\"R\"), full.names = TRUE), source)\ntar_option_set(packages = packages())\n#&gt; Establish _data_analyze.R and _data_analyze_r/globals/globals.R."
  },
  {
    "objectID": "scripts/analysis_scripts/data_analyze.html#code",
    "href": "scripts/analysis_scripts/data_analyze.html#code",
    "title": "9  Analyze data",
    "section": "9.2 Code",
    "text": "9.2 Code\n\nlist(\n    tar_target(\n        name = test2,\n        command = sum(1+1)\n    ),\n    tar_target(\n        name = test_analyze5,\n        command = sum(1+1)\n    )\n)\n#&gt; Establish _data_analyze.R and _data_analyze_r/targets/data_analyze-code.R."
  }
]