---
title: "Orchestration File"
format:
  html:
    code-fold: true
    number-sections: true
---

The orchestration file `_main.qmd` can be used to sequentally compile individual subprojects from a single space.

The project is composed of multiple subprojects and is divided into two main sections.
The first section entails the execution of code to obtain raw data from publicly available sources and conduct initial preprocessing.
Conversely, the second section builds upon the preprocessed data, centering on analysis and reporting.
The connection between the two sections is facilitated by an OSF repository with input data.
Consequently, solely the project administrator is responsible for running the first section, while collaborators can concentrate on the subprojects within section two.


Several environmental variables need to be defined in `.Revinron` file.
For working exclusively in @sec-analysis, only the _url_ of the _pinboard_ needs to be defined.

## Part 1

### Data Collection {#sec-data_collect}

[Code in this section needs to be run only by the project administrators.]{style="color: red"}

Here we obtain data from various sources. Because the sources - APIs, websites, url addresses of files - are external to the project and not persistent, it is unlikely that the code will be reproducible in the longterm. Hence, the main purpose of this section is to document the process of how certain datasets were originally obtained, but it is not expected that researchers will run this code repeatedly.


```{r}
#| label: data_collect
#| eval: false
# Define the paths to the pipeline generator and the generated pipeline
scripts_data_collect <- here::here(
  "scripts",
  "data_collection_scripts",
  c(
    "data_collect.qmd",
    "_data_collect.R"
  )
)
# Generate the pipeline using the Quarto file
quarto::quarto_render(scripts_data_collect[1], quiet = TRUE)
# Run the generated pipeline
targets::tar_make(script = scripts_data_collect[2], store = "_store/_data_collect")
```

The code in this subsection obtains those datasets that need not be collected via APIs or scraping, because they are distributed as files that can be directly downloaded. Setting url addresses for downloads is therefore necessary. Data are simply downloaded and kept as raw data. Processing is documented separarely in @sec-data_process.

- __Subproject name__:
  - `data_collect` 
- __Requirements__: 
  - Set `URL_CSF=URL` in `.Renviron`.
  - Set `URL_ANR_2010=URL` in `.Renviron`.
  - Set `URL_ANR_2009=URL` in `.Renviron`.
  - Set `URL_CSF=URL` in `.Renviron`.
  - Set `URL_ANR_2010=URL` in `.Renviron`.
  - Set `URL_ANR_2010_P=URL` in `.Renviron`.
  - Set `URL_ANR_2009=URL` in `.Renviron`.
  - Set `URL_ANR_2009_P=URL` in `.Renviron`.
  - Set `URL_EU_EUROPE=URL` in `.Renviron`.
  - Set `URL_EU_2020=URL` in `.Renviron`.
  - Set `URL_EU_FP7=URL` in `.Renviron`.
  - Set `URL_SNSF=URL` in `.Renviron`.
  - Set `API_RUN=TRUE` in `.Renviron` to call APIs.


## Part 2

### Input data {#sec-data_input}


```{r}
#| label: data_input
#| eval: false
# Define the paths to the pipeline generator and the generated pipeline
scripts_data_input <- here::here(
  "scripts",
  "processing_scripts",
  c(
    "data_input.qmd", 
    "_data_input.R"
  )
)
# Generate the pipeline using the Quarto file
quarto::quarto_render(scripts_data_input[1], quiet = TRUE)
# Run the generated pipeline
targets::tar_make(script = scripts_data_input[2], store = "_store/_data_input")
```

- __Subproject name__:
  - `data_process` 
- __Requirements__: 
  - @sec-data_collect must be completed and data must be available via [Open Science Framwork](https://osf.io/)

### Data processing {#sec-data_process}

```{r}
#| label: data_process
#| eval: false
# Define the paths to the pipeline generator and the generated pipeline
scripts_data_process <- here::here(
  "scripts",
  "processing_scripts",
  c("data_process.qmd", "_data_process.R"))
# Generate the pipeline using the Quarto file
quarto::quarto_render(scripts_data_process[1], quiet = TRUE)
# Run the generated pipeline
targets::tar_make(script = scripts_data_process[2], store = "_store/_data_process")
```

- __Subproject name__:
  - `data_process` 
- __Requirements__: 
  - @sec-data_input must be completed.
  
This code needs to be run only if data sources change and need to be updated. You should only compile this subproject if you know what you are doing and why you are doing it.




### Analysis (default) {#sec-analysis}

Code in this section should be run by all collaborators, on all machines. It is connected to the previous section only by dependency on the generated pins.

This is the default pipeline. It serves for analysis of the raw data previously exported onto a pinboard. 


```{r}
#| label: default_targets
#| eval: false
# Generate all pipelines using the Quarto project
# defined in _quarto.yaml
quarto::quarto_render(as_job = FALSE)
# Run the generated analytical pipelines
targets::tar_make()
```

- __Subproject name__:
  - `main` 
- __Requirements__:
  - See requirements for @sec-data_analyze.




#### Analyze data {#sec-data_analyze}

- __Subproject name__:
  - `data_analyze` 
- __Requirements__: 
  -  @sec-data_process must be completed

```{r}
#| label: data_analyze
#| eval: false
# Define the paths to the pipeline generator and the generated pipeline
scripts_data_analyze <- here::here(
  "scripts",
  "analysis_scripts",
  c(
    "data_analyze.qmd", 
    "_data_analyze.R"
  )
)
# Generate the pipeline using the Quarto file
quarto::quarto_render(scripts_data_analyze[1], quiet = TRUE)
# Run the generated pipeline
targets::tar_make(script = scripts_data_analyze[2],
store = "_store/_data_analyze")
```


## Report results

## Publish docs


```{r}
#| label: publish-docs
#| eval: false
#From: https://gist.github.com/cobyism/4730490
quarto::quarto_render(as_job = FALSE)
system("touch _book/.nojekyll")
system("git add . && git commit -m 'Update gh-pages'")
system("git subtree push --prefix _book origin gh-pages")
```

